

\chapter{Introduction}
As research in the fields of autonomous systems and robotics have become more extensive, it is evident that there are a wide range of application for robots with integrated autonomy.
There are rovers, drones and even aquatic robots that are capable of decision making in their own environments.
The tasks that these robots carry out can greatly vary as well.
This variance can cause a demand for distinct software and hardware to achieve each robotâ€™s given task.
However, almost all autonomous robots operate similarly through their use of observation (typically with external sensors) and analytics of the data that is observed.

A great deal of research has been done in hybrid robots and creating hardware that is multifunctional to various tasks
However, there is not an extensive amount of research on software with the capability to integrate with multiple robot compositions and tasks.
Most of this is due to the fact that each robot has unique capabilities that do not overlap with many other robots.
Autonomous robots seem to focus in on a certain niche and require their systems to be built from the ground up each time.
This leaves the question of what pieces of autonomous control can be abstracted.

There are many evolutionary computing approaches that can be applied to decision making processes.
These methods are commonly used in situations when there are a known number of controllable variables and a wide solution space to be explored.
This makes them great candidates for creating a system which drives the decision-making process of autonomous robots.
In particular, neural networks and deep neural networks trained in simulations seem to be a promising architecture for finding optimal control patterns in the diverse applications of autonomous robots.

This project approaches the problem from the bottom up.
It looks at the very basics of autonomous robotics.
This is: the collection of data from sensors, analytics of incoming data, and the output of response controls.
Additionally, these three steps are repetitively being performed to achieve a given objective.
I have broken this project into three phases.
The first phase involves setting up a simulation environment to be used for training the autonomous system.
Next, a graphical interphase will be integrated with the simulation data to allow for easy debugging.
Finally, an Artificially Intelligent system will be trained to take in various sets of environmental data as inputs, make decisions based on these inputs and its current objective, and produce a response.

The project is still a work in progress and this paper will only present phases one and two.
These phases cover the procedural environment generator and the graphical interface that pairs with it.
The implementation of the abstracted autonomous system will come in future work.
The main topic covered looks at the representation and formulaic production of data that will be used to represent an environment.
The graphical interfaces capabilities will also be touched on.
For the AI component, we will look at the various evolutionary methods that hold great potential for our given problem setup.
