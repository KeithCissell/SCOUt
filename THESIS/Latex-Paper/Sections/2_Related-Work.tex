

\chapter{Related Work}
The field of autonomous robotics has recently gained a high amount of attention inside and outside of the research community.
As hardware capabilities and intelligent computational techniques have continued to advance, the use of robotics is being introduced to more complex tasks.
Robotic agents continue to phase out humans agents for tasks that are considered mundane or dangerous, as well as for performance reasons where a machine can provide adequate or better results for less cost.
Here we will focus on the application of autonomous robots in surveillance based operations.
The primary examples of surveillance based operations that we will look at are exploration based scientific research and search and rescue settings.

These two types of operations have shown promising boosts in performance through the use of autonomous agents for a few reasons.
Most notably, there are typically certain levels of hazard involved that limit the capability of a human agent and sometimes prevent them entirely.
In the majority of cases, a robotic agent is less susceptible to the same environmental hazards as a human.
As well as objectively increasing agent durability and performance capabilities, use of robotics eliminates the risk of injury, disease and death of any human(s) involved in the operation.
The other advantages to using autonomous robots is the diverse amount of sensors that an robotic agent can use, and their ability to analyze data quickly and without bias.
Sensors, such as an infrared camera, can collect data that humans do not have the capability to observe themselves.
Large amounts of sensor data can also be processed and analyzed by a computer much quicker than humans.
This supports the idea that a robotic agent can perform at higher levels than a human when performing surveillance based operations.

% Use of autonomous robots in exploration settings
There is an abundant amount of existing research on the use of autonomous robots for exploration
\cite{christensen_multi-robot_2017, tai_autonomous_2017, stachniss_exploration_2004, clark_mobile_2007, perea_strom_robust_2017, fink_tier-scalable_2007, bai_toward_2017}.
Each of these autonomous exploration based research experiments share three key components.
There is a robotic agent with a set of actions that can be performed, the agent must use intelligence to navigate their environment, and the agent is goal driven.
While these three components are present in all of their experiments, there is a large variation in the uses and control schemas.
Some are designed for mapping indoor or outdoor environments \cite{tai_autonomous_2017,  stachniss_exploration_2004, perea_strom_robust_2017}, others focus on hazard avoidance \cite{christensen_multi-robot_2017, fink_tier-scalable_2007}, some use multiple agents working together \cite{christensen_multi-robot_2017, clark_mobile_2007}, and some focus on operation efficiency \cite{bai_toward_2017}.
A variety of intelligent approaches for controlling the agent(s) range from the use of Bayesian prediction models \cite{christensen_multi-robot_2017}, Neural Networks \cite{tai_autonomous_2017} and machine learning \cite{bai_toward_2017} to name a few.
Each of these experiments were hand crafted for their specific goal at hand.
The SCOUt project aims to encompass all of these use cases for autonomous agents through the use of one universal process and control schema.
% \cite{christensen_multi-robot_2017} Multi-robots in hazardous areas; Use Bayesian prediction model to avoid hazards.
% \cite{tai_autonomous_2017} Indoor exploration in unknown environment using a Convolutional Neural Network.
% \cite{stachniss_exploration_2004} Combination of autonomous exploration with localization mapping.
% \cite{clark_mobile_2007} Multi-robot perimeter detection.
% \cite{perea_strom_robust_2017} Exploring and mapping unknown environments.
% \cite{fink_tier-scalable_2007} Example robotics mission that requires exploration in hazardous environments.
% \cite{bai_toward_2017} Uses supervised learning for autonomously exploration with efficient user of a single sensor.

While my research did not uncover any existing work on a universal process for project setup, the idea of a single adaptive controller for completing multiple goals is not a new concept.
Other researchers \cite{arora_approach_2017, hutter_online_2018} have created control schemas that are both adaptive to an agent's capabilities as well as varieties of environments.
In \cite{arora_approach_2017}, they use a high level approach for a controller to model and reason with scientific data to generate task based approaches to multiple goals.
Research in \cite{hutter_online_2018} achieves efficient path planning and sensor usage policies through an adaptive Bayesian framework.
SCOUt's intelligent controller is used to achieve similar functionalities as both of the above systems.
% \cite{arora_approach_2017} Use of on board systems to model scientific data and reason path/action planning.
% \cite{hutter_online_2018} Exploration and sensor planning for scientific missions.

The control schema for SCOUt uses memory based reinforcement learning to decide the best possible action to take in a given state to minimize damage to the agent, energy usage and time elapse.
Memory based control schemas have been used in existing experiments \cite{fu_genetic_2003, yi_new_2011} for decision based models.
Both of these experiments used a genetic algorithm (GA) to generate decision policies through the use of existing knowledge (memory).
Reinforcement learning is heavily used in the field of robotics.
Specifically, \cite{arulkumaran_brief_2017, bai_toward_2017, kiumarsi_optimal_2018} all implement reinforcement based learning for action decision models.
\cite{arulkumaran_brief_2017} also covers the handling of large memory sets as growing the memory pool in size can yield more accurate results, but lead to greater complexity and the memory storage management issues involved.
SCOUt's decision schema follows a similar model to \cite{kiumarsi_optimal_2018}, using an adaptive process of choosing an action based on a reinforcement learned score system, critiquing the actors performance in an environment, and updating the action selection process.
% \cite{arulkumaran_brief_2017} Discusses the use of Deep Reinforcement Learning for "experience-driven autonomous learning", pairing with robotics and the challenges related to the complexity of memory, sampling and computation.
% \cite{fu_genetic_2003} GA approach to decision tree building for intelligent action pattern building.
% \cite{yi_new_2011} Another GA approach to decision tree building.
% \cite{kiumarsi_optimal_2018} Very similar action reward system for machine learning using actor -> environment -> critique -> reward.
